{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f2b485f",
   "metadata": {},
   "source": [
    "### Prerequisite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cde674",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r './TOATOD/requirements.txt' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb06b770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1mLet's setup this directory for W&B!\u001b[0m\n",
      "Enter a name for your first project: ^C\n",
      "Aborted!\n"
     ]
    }
   ],
   "source": [
    "# (type in terminal)  \n",
    "# !pip install wandb\n",
    "# !wandb login    \n",
    "# !wandb init     # create project name  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8e8771",
   "metadata": {},
   "source": [
    "## Download DATA\n",
    "- DST, NLG: MultiWOZ 2.1 & MultiWOZ2.2\n",
    "- NLU (intent prediction): Banking77, CLINIC150, HWU64 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6773b35b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOATOD\tTOATOD-modeling.ipynb\n"
     ]
    }
   ],
   "source": [
    "# import os    # change the current working directory  \n",
    "# os.chdir('./TOATOD')\n",
    "!dir  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2274d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 0: cd: data/multiwoz21: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# !cd CUAI6th_1/YuminKim/TOD/TOATOD_git/TOATOD/data/multiwoz21\n",
    "# bash data_preparation.sh  \n",
    "\n",
    "# !cd CUAI6th_1/YuminKim/TOD/TOATOD_git/TOATOD/data/multiwoz22\n",
    "# bash data_preparation.sh\n",
    "\n",
    "# !cd CUAI6th_1/YuminKim/TOD/TOATOD_git/TOATOD/data/banking77\n",
    "# bash banking77_preparation.sh\n",
    "\n",
    "# !cd CUAI6th_1/YuminKim/TOD/TOATOD_git/TOATOD/data/clinc150\n",
    "# bash clinc150_preparation.sh\n",
    "\n",
    "# !cd CUAI6th_1/YuminKim/TOD/TOATOD_git/TOATOD/data/multiwoz21\n",
    "# bash hwu64_preparation.sh \n",
    "\n",
    "# Download Pre-trained Weights: Pass "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fd8d9d",
   "metadata": {},
   "source": [
    "## TOATOD: T5 Generation Model\n",
    "- small model (-->)\n",
    "- base model (NOT YET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44e629b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cd CUAI6th_1/YuminKim/TOD/TOATOD_git/TOATOD/E2E_TOD\n",
    "# bash small_run_21.sh \n",
    "# bash small_run_22.sh "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2ee7dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn  \n",
    "import torch.nn.functional as F \n",
    "from transformers import T5ForConditionalGeneration, T5Config  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbb680b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class T5Gen_Model(nn.Module): \n",
    "    def __init__(self, model_path, tokenizer, dropout=0.1, is_training=True):\n",
    "        super().__init__() \n",
    "        self.tokenizer = tokenizer # tokenizer with extended vocabulary\n",
    "        self.pad_token_id, self.sos_d_token_id, self.eos_d_token_id = self.tokenizer.convert_tokens_to_ids(['<_PAD_>', '<sos_d>', '<eos_d>'])\n",
    "\n",
    "        if is_training:\n",
    "            print ('Initializing Huggingface T5 model...')\n",
    "            t5_config = T5Config.from_pretrained(model_path)\n",
    "            t5_config.__dict__[\"dropout\"] = dropout\n",
    "            self.model = T5ForConditionalGeneration.from_pretrained(model_path, config=t5_config, resume_download=True)\n",
    "        else:    \n",
    "            print('Loading Model from pretrained ckpt...')\n",
    "            self.model = torch.load(os.path.join(model_path, \"model.pt\"))\n",
    "        print ('Resizing Token Embeddings...')\n",
    "\n",
    "        self.model.resize_token_embeddings(len(self.tokenizer))\n",
    "        self.tgt_sos_token_id = self.tokenizer.convert_tokens_to_ids(['<sos_d>'])[0]\n",
    "        self.tgt_eos_token_id = self.tokenizer.convert_tokens_to_ids(['<eos_d>'])[0]\n",
    "\n",
    "    def forward(self, src_input, src_mask, tgt_input, tgt_output):\n",
    "        src_mask = src_mask.type(src_input.type())\n",
    "        outputs = self.model(input_ids=src_input, attention_mask=src_mask, decoder_input_ids=tgt_input, labels=tgt_output)\n",
    "        loss = outputs[0]    # .mean()\n",
    "        return loss \n",
    "         \n",
    "    def parse_batch_text(self, batch_pred_ids):\n",
    "        res_text_list = []\n",
    "        for predicted_ids in batch_pred_ids:  \n",
    "            one_pred_ids = []\n",
    "            for one_id in predicted_ids:\n",
    "                if one_id in [self.pad_token_id, self.sos_d_token_id, self.eos_d_token_id]:\n",
    "                    pass\n",
    "                else:\n",
    "                    one_pred_ids.append(one_id)\n",
    "            one_res_text = self.tokenizer.decode(one_pred_ids)\n",
    "            res_text_list.append(one_res_text)\n",
    "        return res_text_list   \n",
    "\n",
    "    def batch_prediction(self, src_input, src_mask):\n",
    "        # outputs = self.model.generate(input_ids = src_input, attention_mask = src_mask, decoder_start_token_id = self.sos_b_token_id,\n",
    "        #    pad_token_id = self.pad_token_id, eos_token_id = self.eos_b_token_id, max_length = 64)\n",
    "        outputs = self.model.generate(input_ids = src_input, attention_mask = src_mask, decoder_start_token_id = self.tgt_sos_token_id,\n",
    "            pad_token_id = self.pad_token_id, eos_token_id = self.tgt_eos_token_id, max_length = 64)\n",
    "        return self.parse_batch_text(outputs)\n",
    "\n",
    "    def save_model(self, ckpt_save_path):\n",
    "        if not os.path.exists(ckpt_save_path):\n",
    "            os.mkdir(ckpt_save_path)\n",
    "        # save model\n",
    "        torch.save(self.model, os.path.join(ckpt_save_path, 'model.pt'))\n",
    "        # save tokenizer\n",
    "        self.tokenizer.save_pretrained(ckpt_save_path) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bde90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reinforcement Training \n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname(__file__))))\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from transformers import T5Tokenizer\n",
    "from t5adapter import set_task_for_inference, set_task_for_train   \n",
    "\n",
    "class T5ForReinforce(nn.Module):\n",
    "    def __init__(self, model_path, evaluator, special_token_list, alpha=0.7, beta=0.5):\n",
    "        super().__init__()\n",
    "        self.tokenizer = T5Tokenizer.from_pretrained(model_path)\n",
    "        self.model = torch.load(os.path.join(model_path, 'model.pt'),map_location='cpu')\n",
    "        self.evaluator = evaluator\n",
    "        self.special_token_list = special_token_list\n",
    "        self.add_special_decoder_token = True\n",
    "        self.pad_token_id, self.sos_b_token_id, self.eos_b_token_id, self.sos_a_token_id, self.eos_a_token_id, \\\n",
    "        self.sos_r_token_id, self.eos_r_token_id = self.tokenizer.convert_tokens_to_ids(['<_PAD_>', '<sos_b>',\n",
    "                                                                                         '<eos_b>', '<sos_a>',\n",
    "                                                                                         '<eos_a>', '<sos_r>',\n",
    "                                                                                         '<eos_r>'])      \n",
    "\n",
    "        self.alpha = alpha  \n",
    "        self.beta = beta    \n",
    "        self.cross_entropy_loss = nn.CrossEntropyLoss()\n",
    "        self.rewards = []\n",
    "\n",
    "    def forward(self, batch, mode, dial_id=None,dials=None,ver='2.1'):\n",
    "        loss = 0\n",
    "        beta = self.beta\n",
    "        if mode == 'nlg':\n",
    "            start_token, end_token, start_token_id, end_token_id = '<sos_r>', '<eos_r>', self.sos_r_token_id, self.eos_r_token_id\n",
    "\n",
    "            pack = []   \n",
    "            need_key = [\"bspn\",\"dspn\",\"pointer\"]\n",
    "\n",
    "            src_input, src_mask, tgt_input, tgt_output = batch\n",
    "            outputs = self.model(input_ids=src_input, attention_mask=src_mask, labels=tgt_output)\n",
    "            session_loss, logits = outputs.loss, outputs.logits\n",
    "            prob = F.softmax(logits, dim=-1)\n",
    "            loss += session_loss.mean()\n",
    "            batch_size = src_input.size(0)\n",
    "            loss_tensor = torch.zeros(batch_size).to(src_input.device)\n",
    "            for i in range(batch_size):\n",
    "                prediction = self.tokenized_decode(prob[i, :, :].argmax(dim=-1). \\\n",
    "                                                   tolist()).strip()\n",
    "                prediction = prediction.split(start_token)[-1].split(end_token)[0].strip()\n",
    "                preds = []\n",
    "                for token in prediction.split():\n",
    "                    if token == '<_PAD_>':\n",
    "                        continue\n",
    "                    else:\n",
    "                        preds.append(token)\n",
    "                prediction = ' '.join(preds).strip()\n",
    "\n",
    "                golden = tgt_output[i, :].tolist()\n",
    "                golden = golden[:golden.index(-100) if -100 in golden else len(golden)]\n",
    "                gt = self.tokenized_decode(golden).strip()\n",
    "                gt = gt.split(start_token)[-1].split(end_token)[0].strip()\n",
    "\n",
    "                gs = []\n",
    "                for token in gt.split():\n",
    "                    if token == '<_PAD_>':\n",
    "                        continue\n",
    "                    else:\n",
    "                        gs.append(token)\n",
    "                gt = ' '.join(gs)\n",
    "\n",
    "                dic = {}\n",
    "                for key in need_key:\n",
    "                    if not isinstance(dials[i][key],str):\n",
    "                        v = self.tokenized_decode(dials[i][key])\n",
    "                    else:\n",
    "                        v = dials[i][key]\n",
    "                    if key in [\"bspn\"]:\n",
    "                        dic[f\"{key}_gen\"] = v\n",
    "                    else:\n",
    "                        dic[key] = v\n",
    "                dic.update({'dial_id': dial_id[i], 'turn_num': i, 'resp': gt, 'resp_gen': prediction})\n",
    "                pack.append(dic)\n",
    "\n",
    "                p = prob[i, :, :].max(dim=-1).values.prod() + 1e-10\n",
    "                log_prob = torch.log(p)\n",
    "                loss_tensor[i] = log_prob\n",
    "\n",
    "                bleu, success, match = self.evaluator.validation_metric(pack)\n",
    "            # else:\n",
    "            #     results = self.evaluator.e.evaluate(pack)\n",
    "            #     match, success, bleu = results['success']['inform']['total'], results['success']['success']['total'], \\\n",
    "            #                            results['bleu']['mwz22']\n",
    "            # print(prediction)\n",
    "            # print(bleu)\n",
    "            combined_score = 0.5 * (success + match) + bleu\n",
    "            reward = beta * success + (1 - beta) * bleu + 1 # 1 is for avoiding zero reward\n",
    "            loss_tensor = -(loss_tensor * reward / 100) # 100 is for normalization for balancing with categorical cross entropy loss\n",
    "            loss_tensor = loss_tensor.mean()\n",
    "            policy_loss = loss_tensor\n",
    "\n",
    "            loss = self.alpha * policy_loss + (1 - self.alpha) * loss\n",
    "\n",
    "            return loss, \\\n",
    "                   torch.Tensor([reward]).to(loss.device), \\\n",
    "                   torch.Tensor([match]).to(loss.device), \\\n",
    "                   torch.Tensor([success]).to(loss.device), \\\n",
    "                   torch.Tensor([bleu]).to(loss.device), \\\n",
    "                   torch.Tensor([combined_score]).to(loss.device)  \n",
    "\n",
    "        elif mode == 'dst':\n",
    "            start_token, end_token, start_token_id, end_token_id = '<sos_b>', '<eos_b>', self.sos_b_token_id, self.eos_b_token_id\n",
    "            src_input, src_mask, tgt_input, tgt_output = batch\n",
    "            outputs = self.model(input_ids=src_input, attention_mask=src_mask, decoder_input_ids=tgt_input,\n",
    "                                 labels=tgt_output)\n",
    "            loss, logits = outputs.loss, outputs.logits\n",
    "            prob = F.softmax(logits, dim=-1)\n",
    "\n",
    "            batch_size = src_input.size(0)\n",
    "\n",
    "            loss_tensor = torch.zeros(batch_size).to(loss.device)\n",
    "            reward_tensor = torch.zeros(batch_size).to(loss.device)\n",
    "            for i in range(batch_size):\n",
    "                prediction = self.tokenized_decode(prob[i, :, :].argmax(dim=-1).tolist()).strip()\n",
    "                prediction = prediction.split(start_token)[-1].split(end_token)[0].strip()\n",
    "\n",
    "                preds = []\n",
    "                for token in prediction.split():\n",
    "                    if token == '<_PAD_>':\n",
    "                        continue\n",
    "                    else:\n",
    "                        preds.append(token)\n",
    "                prediction = ' '.join(preds)\n",
    "                # prediction to the most of the followings to go and \n",
    "\n",
    "                golden = tgt_output[i, :].tolist()\n",
    "                golden = golden[:golden.index(-100) if -100 in golden else len(golden)]\n",
    "                gt = self.tokenized_decode(golden).strip()\n",
    "                gt = gt.split(start_token)[-1].split(end_token)[0].strip()\n",
    "\n",
    "                gs = []\n",
    "                for token in gt.split():\n",
    "                    if token == '<_PAD_>':\n",
    "                        continue\n",
    "                    else:\n",
    "                        gs.append(token)\n",
    "                gt = ' '.join(gs)\n",
    "\n",
    "                if \"<eos_b>\" in prediction:\n",
    "                    prediction = prediction[:prediction.index(\"<eos_b>\")]\n",
    "                if \"<eos_b>\" in gt:\n",
    "                    gt = gt[:gt.index(\"<eos_b>\")]  \n",
    "\n",
    "                pack = [{\"dial_id\": \"0\", \"turn_num\": 0, \"bspn_gen\": \"\", \"bspn\": \"\"}\n",
    "                    , {\"dial_id\": \"0\", \"turn_num\": str(i + 1), \"bspn_gen\": prediction, \"bspn\": gt}]\n",
    "                rew, f1, acc, _, _ = self.evaluator.dialog_state_tracking_eval(pack, eval_dial_list=[\"0.json\"])\n",
    "                reward = rew + 1  # add 1 to avoid zero reward\n",
    "                p = prob[i, :, :].max(dim=-1).values.prod() + 1e-10\n",
    "\n",
    "                log_prob = torch.log(p)\n",
    "\n",
    "                policy_loss = - (log_prob * reward)\n",
    "                loss_tensor[i] = policy_loss\n",
    "                reward_tensor[i] = rew\n",
    "\n",
    "            r = reward_tensor.mean()\n",
    "            loss_tensor = loss_tensor.mean()\n",
    "            loss = self.alpha * loss_tensor + (1 - self.alpha) * loss\n",
    "            return loss, r\n",
    "\n",
    "    def tokenized_decode(self, token_id_list):\n",
    "        pred_tokens = self.tokenizer.convert_ids_to_tokens(token_id_list)\n",
    "        res_text = ''\n",
    "        curr_list = []\n",
    "        for token in pred_tokens:\n",
    "            if token in self.special_token_list + ['<s>', '</s>', '<pad>']:\n",
    "                if len(curr_list) == 0:\n",
    "                    res_text += ' ' + token + ' '\n",
    "                else:\n",
    "                    curr_res = self.tokenizer.convert_tokens_to_string(curr_list)\n",
    "                    res_text = res_text + ' ' + curr_res + ' ' + token + ' '\n",
    "                    curr_list = []\n",
    "            else:\n",
    "                curr_list.append(token)\n",
    "        if len(curr_list) > 0:\n",
    "            curr_res = self.tokenizer.convert_tokens_to_string(curr_list)\n",
    "            res_text = res_text + ' ' + curr_res + ' '\n",
    "        res_text_list = res_text.strip().split()\n",
    "        res_text = ' '.join(res_text_list).strip()\n",
    "        return res_text\n",
    "\n",
    "    def batch_generate(self, src_input, src_mask, generate_mode, max_decode_len):\n",
    "        '''\n",
    "            This function deals with batch generation. In order to fully take advantage of batch inference,\n",
    "            in each batch, we only generate one type of output. e.g. Given a batch of dialogue history, we\n",
    "            generate the corresponding belief state/dialogue action/system response for the given batch. The\n",
    "            specific type of output is decided by the input argument \"generate_mode\"\n",
    "        '''\n",
    "        if self.add_special_decoder_token:\n",
    "            if generate_mode == 'bs':\n",
    "                start_token, end_token, start_token_id, end_token_id = '<sos_b>', '<eos_b>', self.sos_b_token_id, self.eos_b_token_id\n",
    "            elif generate_mode == 'da':\n",
    "                start_token, end_token, start_token_id, end_token_id = '<sos_a>', '<eos_a>', self.sos_a_token_id, self.eos_a_token_id\n",
    "            elif generate_mode == 'nlg':\n",
    "                start_token, end_token, start_token_id, end_token_id = '<sos_r>', '<eos_r>', self.sos_r_token_id, self.eos_r_token_id\n",
    "            else:\n",
    "                raise Exception('Wrong Generate Mode!!!')\n",
    "        else:\n",
    "            start_token, end_token = '<pad>', '</s>'\n",
    "            start_token_id, end_token_id = \\\n",
    "                self.tokenizer.convert_tokens_to_ids([start_token])[0], \\\n",
    "                self.tokenizer.convert_tokens_to_ids([end_token])[0]\n",
    "\n",
    "        outputs = self.model.generate(input_ids=src_input, attention_mask=src_mask,\n",
    "                                      decoder_start_token_id=start_token_id,\n",
    "                                      pad_token_id=self.pad_token_id, eos_token_id=end_token_id,\n",
    "                                      max_length=max_decode_len)\n",
    "\n",
    "        res_text_list = []\n",
    "        for predicted_ids in outputs:\n",
    "            one_res_text = self.tokenized_decode(predicted_ids)\n",
    "            # print (one_res_text)\n",
    "            one_res_text = one_res_text.split(start_token)[-1].split(end_token)[0].strip()\n",
    "\n",
    "            final_res_list = []\n",
    "            for token in one_res_text.split():\n",
    "                if token == '<_PAD_>':\n",
    "                    continue\n",
    "                else:\n",
    "                    final_res_list.append(token)\n",
    "            one_res_text = ' '.join(final_res_list).strip()\n",
    "\n",
    "            res_text_list.append(one_res_text)\n",
    "        return res_text_list\n",
    "\n",
    "    def save_model(self, ckpt_save_path):\n",
    "        if not os.path.exists(ckpt_save_path):\n",
    "            os.mkdir(ckpt_save_path)\n",
    "        # save model\n",
    "        torch.save(self.model, os.path.join(ckpt_save_path, 'model.pt'))\n",
    "        # save tokenizer\n",
    "        self.tokenizer.save_pretrained(ckpt_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c07016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# E2E_TOD: t5adapter \n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import Parameter\n",
    "from transformers.models.t5.modeling_t5 import T5Stack, T5Block, T5LayerNorm\n",
    "from logging import getLogger\n",
    "logger = getLogger(__name__)\n",
    "\n",
    "class AdapterLayer(nn.Module):\n",
    "    def __init__(self, dim, down_dim, norm=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dim = dim\n",
    "        self.down_dim = down_dim\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.down = nn.Linear(dim, down_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.up = nn.Linear(down_dim, dim)\n",
    "        if norm is not None:\n",
    "            self.layer_norm = T5LayerNorm(dim)\n",
    "            self.layer_norm.weight = Parameter(norm.clone())  \n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.dropout(inputs)\n",
    "        x = self.down(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.up(x)\n",
    "        x += inputs\n",
    "        x = self.layer_norm(x)\n",
    "        return x \n",
    "\n",
    "class TaskOptimizedAdapter(nn.Module):\n",
    "    def __init__(self, adapter_type, adapter_config, task: list, norm=None):\n",
    "        super().__init__()\n",
    "        self.toa = nn.ModuleDict({i: adapter_type(**adapter_config, norm=norm) for i in task})\n",
    "        self.task = 'nlu'\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return self.toa[self.task](inputs)\n",
    "\n",
    "class TaskOptimizedModuleList(nn.ModuleList):\n",
    "    def __init__(self, modules):  \n",
    "        super().__init__()  \n",
    "        self += nn.ModuleList(modules)\n",
    "        self.task = 'nlu'\n",
    "    # freeze pretrained parameter and other task adapters of all blocks\n",
    "    def freeze_pretrained(self, task):\n",
    "        self.task = task\n",
    "        for module in self:\n",
    "            module.task = task\n",
    "            module.freeze_pretrained(task)\n",
    "\n",
    "class T5AdapterBlock(T5Block):\n",
    "    def __init__(self, block, config, adapter_type, adapter_config, task: list):\n",
    "        super().__init__(config)\n",
    "        self.layer = block.layer\n",
    "        self.is_decoder = block.is_decoder\n",
    "        for layer in self.layer:\n",
    "            if 'layer_norm' in layer._modules:\n",
    "                norm = layer.layer_norm.weight.clone()\n",
    "                del layer.layer_norm\n",
    "            else:\n",
    "                norm = None\n",
    "            layer.layer_norm = TaskOptimizedAdapter(adapter_type, adapter_config, task, norm)\n",
    "            layer._modules.move_to_end('dropout')\n",
    "        self.task = 'nlu'\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            hidden_states,\n",
    "            attention_mask=None,\n",
    "            position_bias=None,\n",
    "            encoder_hidden_states=None,\n",
    "            encoder_attention_mask=None,\n",
    "            encoder_decoder_position_bias=None,\n",
    "            layer_head_mask=None,\n",
    "            cross_attn_layer_head_mask=None,\n",
    "            past_key_value=None,\n",
    "            use_cache=False,\n",
    "            output_attentions=False,\n",
    "            return_dict=True,\n",
    "    ):\n",
    "\n",
    "        if past_key_value is not None:\n",
    "            if not self.is_decoder:\n",
    "                logger.warning(\"`past_key_values` is passed to the encoder. Please make sure this is intended.\")\n",
    "            expected_num_past_key_values = 2 if encoder_hidden_states is None else 4\n",
    "\n",
    "            if len(past_key_value) != expected_num_past_key_values:\n",
    "                raise ValueError(\n",
    "                    f\"There should be {expected_num_past_key_values} past states. \"\n",
    "                    f\"{'2 (past / key) for cross attention. ' if expected_num_past_key_values == 4 else ''}\"\n",
    "                    f\"Got {len(past_key_value)} past key / value states\"\n",
    "                )\n",
    "\n",
    "            self_attn_past_key_value = past_key_value[:2]\n",
    "            cross_attn_past_key_value = past_key_value[2:]\n",
    "        else:\n",
    "            self_attn_past_key_value, cross_attn_past_key_value = None, None\n",
    "\n",
    "        self_attention_outputs = self.layer[0](\n",
    "            hidden_states,\n",
    "            attention_mask=attention_mask,\n",
    "            position_bias=position_bias,\n",
    "            layer_head_mask=layer_head_mask,\n",
    "            past_key_value=self_attn_past_key_value,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "        )\n",
    "        hidden_states, present_key_value_state = self_attention_outputs[:2]\n",
    "        attention_outputs = self_attention_outputs[2:]  # Keep self-attention outputs and relative position weights\n",
    "\n",
    "        # clamp inf values to enable fp16 training\n",
    "        if hidden_states.dtype == torch.float16 and torch.isinf(hidden_states).any():\n",
    "            clamp_value = torch.finfo(hidden_states.dtype).max - 1000\n",
    "            hidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)\n",
    "\n",
    "        do_cross_attention = self.is_decoder and encoder_hidden_states is not None\n",
    "        if do_cross_attention:\n",
    "            # the actual query length is unknown for cross attention\n",
    "            # if using past key value states. Need to inject it here\n",
    "            if present_key_value_state is not None:\n",
    "                query_length = present_key_value_state[0].shape[2]\n",
    "            else:\n",
    "                query_length = None\n",
    "\n",
    "            cross_attention_outputs = self.layer[1](\n",
    "                hidden_states,\n",
    "                key_value_states=encoder_hidden_states,\n",
    "                attention_mask=encoder_attention_mask,\n",
    "                position_bias=encoder_decoder_position_bias,\n",
    "                layer_head_mask=cross_attn_layer_head_mask,\n",
    "                past_key_value=cross_attn_past_key_value,\n",
    "                query_length=query_length,\n",
    "                use_cache=use_cache,\n",
    "                output_attentions=output_attentions,\n",
    "            )\n",
    "            hidden_states = cross_attention_outputs[0] \n",
    "\n",
    "            # clamp inf values to enable fp16 training\n",
    "            if hidden_states.dtype == torch.float16 and torch.isinf(hidden_states).any():\n",
    "                clamp_value = torch.finfo(hidden_states.dtype).max - 1000\n",
    "                hidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)\n",
    "\n",
    "            # Combine self attn and cross attn key value states\n",
    "            if present_key_value_state is not None:\n",
    "                present_key_value_state = present_key_value_state + cross_attention_outputs[1]\n",
    "\n",
    "            # Keep cross-attention outputs and relative position weights\n",
    "            attention_outputs = attention_outputs + cross_attention_outputs[2:]\n",
    "\n",
    "        # Apply Feed Forward layer\n",
    "        hidden_states = self.layer[-1](hidden_states)\n",
    "\n",
    "        # clamp inf values to enable fp16 training\n",
    "        if hidden_states.dtype == torch.float16 and torch.isinf(hidden_states).any():\n",
    "            clamp_value = torch.finfo(hidden_states.dtype).max - 1000\n",
    "            hidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)\n",
    "\n",
    "        outputs = (hidden_states,)\n",
    "\n",
    "        if use_cache:\n",
    "            outputs = outputs + (present_key_value_state,) + attention_outputs\n",
    "        else:\n",
    "            outputs = outputs + attention_outputs\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    # freeze pretrained params and other task adapters in the block\n",
    "    def freeze_pretrained(self, task):\n",
    "        assert task in ['all', 'nlu', 'dst', 'policy', 'nlg']\n",
    "\n",
    "        if task == 'all':\n",
    "            for layer in self.layer:\n",
    "                for params in layer.layer_norm.parameters():\n",
    "                    params.requires_grad = True\n",
    "        else:\n",
    "            for layer in self.layer:\n",
    "                layer.layer_norm.task = task\n",
    "                for params in layer.layer_norm.toa[task].parameters():\n",
    "                    params.requires_grad = True\n",
    "\n",
    "    def set_layer_task(self, task):\n",
    "        for layer in self.layer:\n",
    "            layer.layer_norm.task = task\n",
    "\n",
    "# add adapter to the Transformer blocks in the model\n",
    "def add_adapter(model, adapter_type, adapter_config, tasks):\n",
    "    model.encoder.block = TaskOptimizedModuleList(\n",
    "        [T5AdapterBlock(block, model.encoder.config, adapter_type, adapter_config, tasks) for block in\n",
    "         model.encoder.block])\n",
    "    model.decoder.block = TaskOptimizedModuleList(\n",
    "        [T5AdapterBlock(block, model.decoder.config, adapter_type, adapter_config, tasks) for block in\n",
    "         model.decoder.block])\n",
    "    return model\n",
    "\n",
    "\n",
    "# freeze pretrained parameter & other task adapters\n",
    "def set_task_for_train(model, task):\n",
    "    for params in model.parameters():\n",
    "        params.requires_grad = False\n",
    "    model.encoder.block.freeze_pretrained(task)\n",
    "    model.decoder.block.freeze_pretrained(task)\n",
    "    return model\n",
    "\n",
    "\n",
    "def set_task_for_inference(model, task):\n",
    "    for block in model.encoder.block:\n",
    "        block.task = task\n",
    "        block.set_layer_task(task)\n",
    "    for block in model.decoder.block:\n",
    "        block.task = task\n",
    "        block.set_layer_task(task)\n",
    "    return model\n",
    "\n",
    "def copy_weight(target_model, reference_model, task):\n",
    "    reference_encoder, reference_decoder = reference_model.encoder, reference_model.decoder\n",
    "    \n",
    "    for block, ref_block in zip(target_model.encoder.block,reference_encoder.block):\n",
    "        for layer, ref_layer in zip(block.layer, ref_block.layer):\n",
    "            for params, ref_params in zip(layer.layer_norm.toa[task].parameters(), ref_layer.layer_norm.toa[task].parameters()):\n",
    "                params.data.copy_(ref_params.data)\n",
    "    for block, ref_block in zip(target_model.decoder.block, reference_decoder.block):\n",
    "        for layer, ref_layer in zip(block.layer, ref_block.layer):\n",
    "            for params, ref_params in zip(layer.layer_norm.toa[task].parameters(), ref_layer.layer_norm.toa[task].parameters()):\n",
    "                params.data.copy_(ref_params.data)\n",
    "\n",
    "    return target_model\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    from transformers import T5ForConditionalGeneration, AutoTokenizer\n",
    "    model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
    "    tokenizer = AutoTokenizer.from_pretrained('t5-base')\n",
    "\n",
    "    # add adapter to the model\n",
    "    model = add_adapter(model, AdapterLayer, {'dim':1024, 'down_dim':256},['nlu', 'dst', 'policy', 'nlg'])\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0049f99b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d62f16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ea59ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc65ec6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
