{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f2b485f",
   "metadata": {},
   "source": [
    "### Prerequisite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cde674",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb06b770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1mLet's setup this directory for W&B!\u001b[0m\n",
      "Enter a name for your first project: ^C\n",
      "Aborted!\n"
     ]
    }
   ],
   "source": [
    "# (type in terminal)  \n",
    "# !pip install wandb\n",
    "# !wandb login    \n",
    "# !wandb init     # create project name  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8e8771",
   "metadata": {},
   "source": [
    "## DATA\n",
    "- DST, NLG: MultiWOZ 2.1 & MultiWOZ2.2\n",
    "- NLU (intent prediction): Banking77, CLINIC150, HWU64 datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aacf9e0",
   "metadata": {},
   "source": [
    "### MultiWOZ 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6e532fcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/work/CUAI6th_1/YuminKim/TOD'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "import json   \n",
    "os.getcwd()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ce0379",
   "metadata": {},
   "outputs": [],
   "source": [
    "mwoz2_test = json.load('TOD_DATA/MultiWOZ_2.2_test.zip')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6aaaf112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 10003\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 3080\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "banking = load_dataset('PolyAI/banking77')  \n",
    "print(banking)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fd8d9d",
   "metadata": {},
   "source": [
    "## TOATOD: T5Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2ee7dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn  \n",
    "import torch.nn.functional as F \n",
    "from transformers import T5ForConditionalGeneration, T5Config  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbb680b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class T5Gen_Model(nn.Module): \n",
    "    def __init__(self, model_path, tokenizer, dropout=0.1, is_training=True):\n",
    "        super().__init__() \n",
    "        self.tokenizer = tokenizer # tokenizer with extended vocabulary\n",
    "        self.pad_token_id, self.sos_d_token_id, self.eos_d_token_id = self.tokenizer.convert_tokens_to_ids(['<_PAD_>', '<sos_d>', '<eos_d>'])\n",
    "\n",
    "        if is_training:\n",
    "            print ('Initializing Huggingface T5 model...')\n",
    "            t5_config = T5Config.from_pretrained(model_path)\n",
    "            t5_config.__dict__[\"dropout\"] = dropout\n",
    "            self.model = T5ForConditionalGeneration.from_pretrained(model_path, config=t5_config, resume_download=True)\n",
    "        else:    \n",
    "            print('Loading Model from pretrained ckpt...')\n",
    "            self.model = torch.load(os.path.join(model_path, \"model.pt\"))\n",
    "        print ('Resizing Token Embeddings...')\n",
    "\n",
    "        self.model.resize_token_embeddings(len(self.tokenizer))\n",
    "        self.tgt_sos_token_id = self.tokenizer.convert_tokens_to_ids(['<sos_d>'])[0]\n",
    "        self.tgt_eos_token_id = self.tokenizer.convert_tokens_to_ids(['<eos_d>'])[0]\n",
    "\n",
    "    def forward(self, src_input, src_mask, tgt_input, tgt_output):\n",
    "        src_mask = src_mask.type(src_input.type())\n",
    "        outputs = self.model(input_ids=src_input, attention_mask=src_mask, decoder_input_ids=tgt_input, labels=tgt_output)\n",
    "        loss = outputs[0]    # .mean()\n",
    "        return loss \n",
    "         \n",
    "    def parse_batch_text(self, batch_pred_ids):\n",
    "        res_text_list = []\n",
    "        for predicted_ids in batch_pred_ids:  \n",
    "            one_pred_ids = []\n",
    "            for one_id in predicted_ids:\n",
    "                if one_id in [self.pad_token_id, self.sos_d_token_id, self.eos_d_token_id]:\n",
    "                    pass\n",
    "                else:\n",
    "                    one_pred_ids.append(one_id)\n",
    "            one_res_text = self.tokenizer.decode(one_pred_ids)\n",
    "            res_text_list.append(one_res_text)\n",
    "        return res_text_list   \n",
    "\n",
    "    def batch_prediction(self, src_input, src_mask):\n",
    "        # outputs = self.model.generate(input_ids = src_input, attention_mask = src_mask, decoder_start_token_id = self.sos_b_token_id,\n",
    "        #    pad_token_id = self.pad_token_id, eos_token_id = self.eos_b_token_id, max_length = 64)\n",
    "        outputs = self.model.generate(input_ids = src_input, attention_mask = src_mask, decoder_start_token_id = self.tgt_sos_token_id,\n",
    "            pad_token_id = self.pad_token_id, eos_token_id = self.tgt_eos_token_id, max_length = 64)\n",
    "        return self.parse_batch_text(outputs)\n",
    "\n",
    "    def save_model(self, ckpt_save_path):\n",
    "        if not os.path.exists(ckpt_save_path):\n",
    "            os.mkdir(ckpt_save_path)\n",
    "        # save model\n",
    "        torch.save(self.model, os.path.join(ckpt_save_path, 'model.pt'))\n",
    "        # save tokenizer\n",
    "        self.tokenizer.save_pretrained(ckpt_save_path)       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bde90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reinforcement Training \n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname(__file__))))\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from transformers import T5Tokenizer\n",
    "from t5adapter import set_task_for_inference, set_task_for_train   \n",
    "\n",
    "class T5ForReinforce(nn.Module):\n",
    "    def __init__(self, model_path, evaluator, special_token_list, alpha=0.7, beta=0.5):\n",
    "        super().__init__()\n",
    "        self.tokenizer = T5Tokenizer.from_pretrained(model_path)\n",
    "        self.model = torch.load(os.path.join(model_path, 'model.pt'),map_location='cpu')\n",
    "        self.evaluator = evaluator\n",
    "        self.special_token_list = special_token_list\n",
    "        self.add_special_decoder_token = True\n",
    "        self.pad_token_id, self.sos_b_token_id, self.eos_b_token_id, self.sos_a_token_id, self.eos_a_token_id, \\\n",
    "        self.sos_r_token_id, self.eos_r_token_id = self.tokenizer.convert_tokens_to_ids(['<_PAD_>', '<sos_b>',\n",
    "                                                                                         '<eos_b>', '<sos_a>',\n",
    "                                                                                         '<eos_a>', '<sos_r>',\n",
    "                                                                                         '<eos_r>'])      \n",
    "        self.alpha = alpha  \n",
    "        self.beta = beta    \n",
    "        self.cross_entropy_loss = nn.CrossEntropyLoss()\n",
    "        self.rewards = []\n",
    "\n",
    "    def forward(self, batch, mode, dial_id=None,dials=None,ver='2.1'):\n",
    "        loss = 0\n",
    "        beta = self.beta\n",
    "        if mode == 'nlg':\n",
    "            start_token, end_token, start_token_id, end_token_id = '<sos_r>', '<eos_r>', self.sos_r_token_id, self.eos_r_token_id\n",
    "\n",
    "            pack = []   \n",
    "            need_key = [\"bspn\",\"dspn\",\"pointer\"]\n",
    "\n",
    "            src_input, src_mask, tgt_input, tgt_output = batch\n",
    "            outputs = self.model(input_ids=src_input, attention_mask=src_mask, labels=tgt_output)\n",
    "            session_loss, logits = outputs.loss, outputs.logits\n",
    "            prob = F.softmax(logits, dim=-1)\n",
    "            loss += session_loss.mean()\n",
    "            batch_size = src_input.size(0)\n",
    "            loss_tensor = torch.zeros(batch_size).to(src_input.device)\n",
    "            for i in range(batch_size):\n",
    "                prediction = self.tokenized_decode(prob[i, :, :].argmax(dim=-1). \\\n",
    "                                                   tolist()).strip()\n",
    "                prediction = prediction.split(start_token)[-1].split(end_token)[0].strip()\n",
    "                preds = []\n",
    "                for token in prediction.split():\n",
    "                    if token == '<_PAD_>':\n",
    "                        continue\n",
    "                    else:\n",
    "                        preds.append(token)\n",
    "                prediction = ' '.join(preds).strip()\n",
    "\n",
    "                golden = tgt_output[i, :].tolist()\n",
    "                golden = golden[:golden.index(-100) if -100 in golden else len(golden)]\n",
    "                gt = self.tokenized_decode(golden).strip()\n",
    "                gt = gt.split(start_token)[-1].split(end_token)[0].strip()\n",
    "\n",
    "                gs = []\n",
    "                for token in gt.split():\n",
    "                    if token == '<_PAD_>':\n",
    "                        continue\n",
    "                    else:\n",
    "                        gs.append(token)\n",
    "                gt = ' '.join(gs)\n",
    "\n",
    "                dic = {}\n",
    "                for key in need_key:\n",
    "                    if not isinstance(dials[i][key],str):\n",
    "                        v = self.tokenized_decode(dials[i][key])\n",
    "                    else:\n",
    "                        v = dials[i][key]\n",
    "                    if key in [\"bspn\"]:\n",
    "                        dic[f\"{key}_gen\"] = v\n",
    "                    else:\n",
    "                        dic[key] = v\n",
    "                dic.update({'dial_id': dial_id[i], 'turn_num': i, 'resp': gt, 'resp_gen': prediction})\n",
    "                pack.append(dic)\n",
    "\n",
    "                p = prob[i, :, :].max(dim=-1).values.prod() + 1e-10\n",
    "                log_prob = torch.log(p)\n",
    "                loss_tensor[i] = log_prob\n",
    "\n",
    "                bleu, success, match = self.evaluator.validation_metric(pack)\n",
    "            # else:\n",
    "            #     results = self.evaluator.e.evaluate(pack)\n",
    "            #     match, success, bleu = results['success']['inform']['total'], results['success']['success']['total'], \\\n",
    "            #                            results['bleu']['mwz22']\n",
    "            # print(prediction)\n",
    "            # print(bleu)\n",
    "            combined_score = 0.5 * (success + match) + bleu\n",
    "            reward = beta * success + (1 - beta) * bleu + 1 # 1 is for avoiding zero reward\n",
    "            loss_tensor = -(loss_tensor * reward / 100) # 100 is for normalization for balancing with categorical cross entropy loss\n",
    "            loss_tensor = loss_tensor.mean()\n",
    "            policy_loss = loss_tensor\n",
    "\n",
    "            loss = self.alpha * policy_loss + (1 - self.alpha) * loss\n",
    "\n",
    "            return loss, \\\n",
    "                   torch.Tensor([reward]).to(loss.device), \\\n",
    "                   torch.Tensor([match]).to(loss.device), \\\n",
    "                   torch.Tensor([success]).to(loss.device), \\\n",
    "                   torch.Tensor([bleu]).to(loss.device), \\\n",
    "                   torch.Tensor([combined_score]).to(loss.device)  \n",
    "\n",
    "        elif mode == 'dst':\n",
    "            start_token, end_token, start_token_id, end_token_id = '<sos_b>', '<eos_b>', self.sos_b_token_id, self.eos_b_token_id\n",
    "            src_input, src_mask, tgt_input, tgt_output = batch\n",
    "            outputs = self.model(input_ids=src_input, attention_mask=src_mask, decoder_input_ids=tgt_input,\n",
    "                                 labels=tgt_output)\n",
    "            loss, logits = outputs.loss, outputs.logits\n",
    "            prob = F.softmax(logits, dim=-1)\n",
    "\n",
    "            batch_size = src_input.size(0)\n",
    "\n",
    "            loss_tensor = torch.zeros(batch_size).to(loss.device)\n",
    "            reward_tensor = torch.zeros(batch_size).to(loss.device)\n",
    "            for i in range(batch_size):\n",
    "                prediction = self.tokenized_decode(prob[i, :, :].argmax(dim=-1).tolist()).strip()\n",
    "                prediction = prediction.split(start_token)[-1].split(end_token)[0].strip()\n",
    "\n",
    "                preds = []\n",
    "                for token in prediction.split():\n",
    "                    if token == '<_PAD_>':\n",
    "                        continue\n",
    "                    else:\n",
    "                        preds.append(token)\n",
    "                prediction = ' '.join(preds)\n",
    "                # prediction to the most of the followings to go and \n",
    "\n",
    "                golden = tgt_output[i, :].tolist()\n",
    "                golden = golden[:golden.index(-100) if -100 in golden else len(golden)]\n",
    "                gt = self.tokenized_decode(golden).strip()\n",
    "                gt = gt.split(start_token)[-1].split(end_token)[0].strip()\n",
    "\n",
    "                gs = []\n",
    "                for token in gt.split():\n",
    "                    if token == '<_PAD_>':\n",
    "                        continue\n",
    "                    else:\n",
    "                        gs.append(token)\n",
    "                gt = ' '.join(gs)\n",
    "\n",
    "                if \"<eos_b>\" in prediction:\n",
    "                    prediction = prediction[:prediction.index(\"<eos_b>\")]\n",
    "                if \"<eos_b>\" in gt:\n",
    "                    gt = gt[:gt.index(\"<eos_b>\")]\n",
    "\n",
    "                pack = [{\"dial_id\": \"0\", \"turn_num\": 0, \"bspn_gen\": \"\", \"bspn\": \"\"}\n",
    "                    , {\"dial_id\": \"0\", \"turn_num\": str(i + 1), \"bspn_gen\": prediction, \"bspn\": gt}]\n",
    "                rew, f1, acc, _, _ = self.evaluator.dialog_state_tracking_eval(pack, eval_dial_list=[\"0.json\"])\n",
    "                reward = rew + 1  # add 1 to avoid zero reward\n",
    "                p = prob[i, :, :].max(dim=-1).values.prod() + 1e-10\n",
    "\n",
    "                log_prob = torch.log(p)\n",
    "\n",
    "                policy_loss = - (log_prob * reward)\n",
    "                loss_tensor[i] = policy_loss\n",
    "                reward_tensor[i] = rew\n",
    "\n",
    "            r = reward_tensor.mean()\n",
    "            loss_tensor = loss_tensor.mean()\n",
    "            loss = self.alpha * loss_tensor + (1 - self.alpha) * loss\n",
    "            return loss, r\n",
    "\n",
    "    def tokenized_decode(self, token_id_list):\n",
    "        pred_tokens = self.tokenizer.convert_ids_to_tokens(token_id_list)\n",
    "        res_text = ''\n",
    "        curr_list = []\n",
    "        for token in pred_tokens:\n",
    "            if token in self.special_token_list + ['<s>', '</s>', '<pad>']:\n",
    "                if len(curr_list) == 0:\n",
    "                    res_text += ' ' + token + ' '\n",
    "                else:\n",
    "                    curr_res = self.tokenizer.convert_tokens_to_string(curr_list)\n",
    "                    res_text = res_text + ' ' + curr_res + ' ' + token + ' '\n",
    "                    curr_list = []\n",
    "            else:\n",
    "                curr_list.append(token)\n",
    "        if len(curr_list) > 0:\n",
    "            curr_res = self.tokenizer.convert_tokens_to_string(curr_list)\n",
    "            res_text = res_text + ' ' + curr_res + ' '\n",
    "        res_text_list = res_text.strip().split()\n",
    "        res_text = ' '.join(res_text_list).strip()\n",
    "        return res_text\n",
    "\n",
    "    def batch_generate(self, src_input, src_mask, generate_mode, max_decode_len):\n",
    "        '''\n",
    "            This function deals with batch generation. In order to fully take advantage of batch inference,\n",
    "            in each batch, we only generate one type of output. e.g. Given a batch of dialogue history, we\n",
    "            generate the corresponding belief state/dialogue action/system response for the given batch. The\n",
    "            specific type of output is decided by the input argument \"generate_mode\"\n",
    "        '''\n",
    "        if self.add_special_decoder_token:\n",
    "            if generate_mode == 'bs':\n",
    "                start_token, end_token, start_token_id, end_token_id = '<sos_b>', '<eos_b>', self.sos_b_token_id, self.eos_b_token_id\n",
    "            elif generate_mode == 'da':\n",
    "                start_token, end_token, start_token_id, end_token_id = '<sos_a>', '<eos_a>', self.sos_a_token_id, self.eos_a_token_id\n",
    "            elif generate_mode == 'nlg':\n",
    "                start_token, end_token, start_token_id, end_token_id = '<sos_r>', '<eos_r>', self.sos_r_token_id, self.eos_r_token_id\n",
    "            else:\n",
    "                raise Exception('Wrong Generate Mode!!!')\n",
    "        else:\n",
    "            start_token, end_token = '<pad>', '</s>'\n",
    "            start_token_id, end_token_id = \\\n",
    "                self.tokenizer.convert_tokens_to_ids([start_token])[0], \\\n",
    "                self.tokenizer.convert_tokens_to_ids([end_token])[0]\n",
    "\n",
    "        outputs = self.model.generate(input_ids=src_input, attention_mask=src_mask,\n",
    "                                      decoder_start_token_id=start_token_id,\n",
    "                                      pad_token_id=self.pad_token_id, eos_token_id=end_token_id,\n",
    "                                      max_length=max_decode_len)\n",
    "\n",
    "        res_text_list = []\n",
    "        for predicted_ids in outputs:\n",
    "            one_res_text = self.tokenized_decode(predicted_ids)\n",
    "            # print (one_res_text)\n",
    "            one_res_text = one_res_text.split(start_token)[-1].split(end_token)[0].strip()\n",
    "\n",
    "            final_res_list = []\n",
    "            for token in one_res_text.split():\n",
    "                if token == '<_PAD_>':\n",
    "                    continue\n",
    "                else:\n",
    "                    final_res_list.append(token)\n",
    "            one_res_text = ' '.join(final_res_list).strip()\n",
    "\n",
    "            res_text_list.append(one_res_text)\n",
    "        return res_text_list\n",
    "\n",
    "    def save_model(self, ckpt_save_path):\n",
    "        if not os.path.exists(ckpt_save_path):\n",
    "            os.mkdir(ckpt_save_path)\n",
    "        # save model\n",
    "        torch.save(self.model, os.path.join(ckpt_save_path, 'model.pt'))\n",
    "        # save tokenizer\n",
    "        self.tokenizer.save_pretrained(ckpt_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c07016",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
